{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Englishson0909/2024spring/blob/main/0403practice2Corpus/TTR-and-lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌿 Topics:\n",
        "\n",
        "## 1. **Type vs. Token**\n",
        "## 2. Lexical Diversity measures (10 types)"
      ],
      "metadata": {
        "id": "X1fv4aZ0B68x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Type vs. Token\n",
        "\n",
        "Example: A cat is chasing a mouse.\n",
        "\n",
        "+ Tokens: Tokens are often words, but they can also include punctuation, numbers, and other characters depending on the analysis. Simply put, tokens are the total number of words in a given text.\n",
        "\n",
        "  + 6 tokens in the given example\n",
        "\n",
        "+ Types: A type is the unique form of a token, disregarding its frequency of occurrence.\n",
        "\n",
        "  + 5 types in the given example."
      ],
      "metadata": {
        "id": "QuJHFaewBq5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[text samples from Aesop fables](https://aesopsfables.org/)"
      ],
      "metadata": {
        "id": "QKFZ5TxMP5ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈스페이도 하나의 아이템 len 함수에서는 빈칸도 계산한다"
      ],
      "metadata": {
        "id": "BDhxR3PNjCr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myword[3]"
      ],
      "metadata": {
        "id": "wA9Ydmmagn-T",
        "outputId": "37793223-a30b-4fa6-e8ca-477f31b43bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'l'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(myword)"
      ],
      "metadata": {
        "id": "Hm_89u-4g2Zt",
        "outputId": "e6abb8d6-9c8c-4591-d432-e3a9b0dc7b40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"An ant went to a Mistic-fountain.\"\n",
        "len(text)"
      ],
      "metadata": {
        "id": "cxO7aH8JQlXy",
        "outputId": "91743d60-c695-43fc-f762-3f5ab109b2fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(myword))"
      ],
      "metadata": {
        "id": "DgymgjZ9hAic",
        "outputId": "a5ebc7b2-1a37-4dfa-b576-cc4d234a48c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print를 마지막에 넣으면 최종하나만 나온다"
      ],
      "metadata": {
        "id": "tEHzQwE7hPa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myword[3:6]"
      ],
      "metadata": {
        "id": "zZ7pgFQNhTGP",
        "outputId": "c77d520d-47ad-43c9-87e2-d773079a0544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'le'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myword[-3:0]"
      ],
      "metadata": {
        "id": "_q7BYwxEhiw8",
        "outputId": "727cbaa0-74df-44d2-9bdb-e261f71821d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myword=[\"sample\",\"6\",\"apple\"]"
      ],
      "metadata": {
        "id": "CJi72gqBhsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Tokens are often words, but they can also include punctuation, numbers, and other characters depending on the analysis. Simply put, tokens are the total number of words in a given text.\""
      ],
      "metadata": {
        "id": "4J92rfA3ghDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "yySt0Pg-h3sc",
        "outputId": "b46a8db8-c184-4f74-849a-4046b9d7b036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tokens are often words, but they can also include punctuation, numbers, and other characters depending on the analysis. Simply put, tokens are the total number of words in a given text.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "list의 경우 1은 srting단어가 나온다"
      ],
      "metadata": {
        "id": "LMa1Caw_iDR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **text.split()** # split string by space"
      ],
      "metadata": {
        "id": "1yjXhWTJQ6TX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".split 스페이를 스트링을 나눈 것"
      ],
      "metadata": {
        "id": "4SWqOiW2jt9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = text.split()\n",
        "print(step1)"
      ],
      "metadata": {
        "id": "1pc8WHWwQkrp",
        "outputId": "6b046219-ad3d-484e-d4a8-59b5e88965e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokens', 'are', 'often', 'words,', 'but', 'they', 'can', 'also', 'include', 'punctuation,', 'numbers,', 'and', 'other', 'characters', 'depending', 'on', 'the', 'analysis.', 'Simply', 'put,', 'tokens', 'are', 'the', 'total', 'number', 'of', 'words', 'in', 'a', 'given', 'text.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장을 나누는 방법 마침표를 중심으로 나눈다"
      ],
      "metadata": {
        "id": "hUXafbLBkAvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step2 = text.split(\".\") # delimiter here is '.'\n",
        "print(step2)\n",
        "len(step2)"
      ],
      "metadata": {
        "id": "M_qkU6FwREHI",
        "outputId": "49ba718e-9e45-4a8a-9771-eb85f013aa31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokens are often words, but they can also include punctuation, numbers, and other characters depending on the analysis', ' Simply put, tokens are the total number of words in a given text', '']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=input()\n",
        "print(\"N of string:\",len(text))\n",
        "words=text.split()\n",
        "print(len(words))\n",
        "sents=text.split(\".\")\n",
        "print(sents)"
      ],
      "metadata": {
        "id": "P7us1FqHkWOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(myword)"
      ],
      "metadata": {
        "id": "1GvwGmeNiKG_",
        "outputId": "8c2b9164-d37e-41b1-99c7-6127b13f28ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a longer text"
      ],
      "metadata": {
        "id": "UNpznL4oSIGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\" 세개는 에러가 없다\n",
        "\" 하나는 오류다 - 끝에 코드가 연결되었다는 의미 없다\n",
        "''' 가능"
      ],
      "metadata": {
        "id": "OC6vUpQ7lg22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This includes all characters: letters, numbers, spaces, punctuation marks, and special characters.\n",
        "\n",
        "text = \"\"\"\n",
        "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
        "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Number of strings: \", len(text))"
      ],
      "metadata": {
        "id": "nrIqszf5Pj1u",
        "outputId": "c12b1d2a-5069-4359-d8d9-10ac2fadbeee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of strings:  554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어를 ()스페이스로 나눈것 token"
      ],
      "metadata": {
        "id": "ubpDjWVWl2Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()\n",
        "len(tokens)"
      ],
      "metadata": {
        "id": "12i5q8r6QhcX",
        "outputId": "c2cee776-7667-46a1-e63a-c832cda330ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set은 count unique string 똑같은 건 하나로 친다"
      ],
      "metadata": {
        "id": "MmdkLJYLl9Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1=\"banana, apple, apple, ornage, banana\"\n",
        "words=w1.split()"
      ],
      "metadata": {
        "id": "Wgvs0hCPmFiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = set(tokens)\n",
        "len(types)\n",
        "set(words)"
      ],
      "metadata": {
        "id": "LkBDUXDfmpbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = set(tokens)\n",
        "len(types)\n",
        "set(words)"
      ],
      "metadata": {
        "id": "gVRl7ynYShZr",
        "outputId": "2b6864fd-82ce-4f77-c199-3034fdc1a511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function"
      ],
      "metadata": {
        "id": "l0FsA9B5TPgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def로 시작하면 함수 그 이후에 count, token, type은 같은 줄에 넣기"
      ],
      "metadata": {
        "id": "OoEW3DLfm6Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_types_and_tokens(text):\n",
        "    tokens = text.split()\n",
        "    types = set(tokens)\n",
        "    return len(types), len(tokens)"
      ],
      "metadata": {
        "id": "IVV1N3KjPuwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"\"\"\n",
        "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
        "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8iu5gS2HnX1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jHdoqJGtn7Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#@markdown Hidig code 옆에 코드는 안보이고 md만 보임"
      ],
      "metadata": {
        "id": "48zTqHDToiEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Hidig code\n",
        "# Example text\n",
        "\n",
        "num_types, num_tokens = count_types_and_tokens(text)\n",
        "print(\"Number of types:\", num_types)\n",
        "print(\"Number of tokens:\", num_tokens)"
      ],
      "metadata": {
        "id": "-06hNP2eTLdG",
        "outputId": "417d4d38-97d1-4b7b-f492-7381d1d0fc13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of types: 76\n",
            "Number of tokens: 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "+ lemma: a dictionary form or base form of a set of words.\n",
        "+ example: 'run, runs, running, ran' => 'run'\n",
        "\n",
        "We will use {nltk} modules"
      ],
      "metadata": {
        "id": "NQXlTrHOTYBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h6MmYBKyrZWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "BkZIe6KlTzdi",
        "outputId": "522c6cef-1a64-436a-e9d4-3bc2653af40e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download the WordNet resource (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "E8HhAfXZT_Xt",
        "outputId": "65b762c2-2381-41dd-bb91-d9da86bdd403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The function below, get_wordnet_pos, is designed to map the part-of-speech (POS) tags provided by NLTK's pos_tag function to the format that is recognized by the WordNet Lemmatizer, which is part of the NLTK library. This mapping is essential for accurate lemmatization, as it allows the lemmatizer to understand the grammatical category of each word."
      ],
      "metadata": {
        "id": "pBsUOddJWRoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define get_wordnet_pos(word)\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "asSvbbChUMf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The cats are running faster than the dogs\""
      ],
      "metadata": {
        "id": "65oy83YqUriA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Lemmatization using POS tags\n",
        "lemmatized_output = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "print('Original Sentence:', sentence)\n",
        "print('Lemmatized Sentence:', ' '.join(lemmatized_output))"
      ],
      "metadata": {
        "id": "JlfD53I4UhpX",
        "outputId": "f7facae0-700e-47e6-fb89-dcb702c5efa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The cats are running faster than the dogs\n",
            "Lemmatized Sentence: The cat be run faster than the dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization practice with our text"
      ],
      "metadata": {
        "id": "Hrc_6o_DU_MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
        "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kvWYRRrOVFpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Lemmatization using POS tags\n",
        "lemmatized_output = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "print('Original Sentence:', text)\n",
        "print('Lemmatized Sentence:', ' '.join(lemmatized_output))"
      ],
      "metadata": {
        "id": "mnFhi93IVMPg",
        "outputId": "07a187f0-8826-4d74-ec53-475c7c45754f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: \n",
            "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
            "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
            "\n",
            "Lemmatized Sentence: An ant go to a fountain to quench his thirst and , tumble in , be almost drown . But a dove that happen to be sit on a neighbor tree saw the ant 's danger and , pluck off a leaf , let it drop into the water before him . The ant mount upon it , be presently waft safely ashore . Just at that time , a fowler be spread his net and be in the act of ensnare the dove , when the ant , perceive his object , bit his heel . The start this give the man make him drop his net and the dove , arouse to a sense of her danger , flew safely away .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lemmatized_output)"
      ],
      "metadata": {
        "id": "1IcWfn3LXX3C",
        "outputId": "fcad03f5-2f6e-4acc-dc93-d83237924266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare text tokens, types, and lemmatized"
      ],
      "metadata": {
        "id": "q4L-fuHZXd1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
        "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_E_UomUKYeLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split(); print(len(tokens))\n",
        "print(len(lemmatized_output))"
      ],
      "metadata": {
        "id": "SqLZXVAXYiJa",
        "outputId": "8c6b38a2-476a-4ee2-abba-9890215f9ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n",
            "124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types in the text order"
      ],
      "metadata": {
        "id": "H5u8BYYbagUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'tokens' is already defined\n",
        "\n",
        "types_in_order = []\n",
        "seen = set()\n",
        "\n",
        "for token in tokens:\n",
        "    if token not in seen:\n",
        "        seen.add(token)\n",
        "        types_in_order.append(token)\n",
        "\n",
        "# Now 'types_in_order' contains unique elements from 'tokens' in the order they appear in the text\n"
      ],
      "metadata": {
        "id": "upt-2KK1aVpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe with tokens, types_in_order, lemmatized_output\n",
        "\n",
        "print(len(tokens))\n",
        "print(len(types_in_order))\n",
        "print(len(lemmatized_output))"
      ],
      "metadata": {
        "id": "pYqm_ezLZtzP",
        "outputId": "22c47acb-ae63-4004-aa5c-e9d04e28990b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n",
            "76\n",
            "124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pandas는 엑셀 사용"
      ],
      "metadata": {
        "id": "mblelI65wBDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "Wqo2YtsBYIm-",
        "outputId": "6f691b66-0423-45d6-9b90-122000719ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "판다스를 불러와서 데이타 프레임 만들기\n",
        "df= data frame  pd.DataFrame =CSV 파일 만들기"
      ],
      "metadata": {
        "id": "nEs2ckluwVDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming tokens, types_in_order, and lemmatized_output are already defined\n",
        "# and their lengths are 107, 76, 124 respectively\n",
        "\n",
        "# Extend types_in_order and tokens with 'None' to match the length of lemmatized_output\n",
        "types_in_order.extend([None] * (len(lemmatized_output) - len(types_in_order)))\n",
        "tokens.extend([None] * (len(lemmatized_output) - len(tokens)))\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Tokens': tokens,\n",
        "    'Types': types_in_order,\n",
        "    'Lemmatized Output': lemmatized_output\n",
        "})\n",
        "\n",
        "df[1:20]\n"
      ],
      "metadata": {
        "id": "nlzdg1l4YMu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문학 작품을 읽고 다양한 어휘를 사용해서 book journal 써보기\n",
        "문학 작품을 읽고 어휘 표 만들기 /주인공 이름나라 등을 통해 이야기 추론해보기"
      ],
      "metadata": {
        "id": "N-SxMoeBxqRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TTR (Type-to-Token Ratio)"
      ],
      "metadata": {
        "id": "RRvfPCr7VEV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already calculated the number of types and tokens\n",
        "number_of_types = len(types)  # Number of unique words\n",
        "number_of_tokens = len(tokens)  # Total number of words\n",
        "\n",
        "# Calculate TTR\n",
        "TTR = number_of_types / number_of_tokens\n",
        "\n",
        "print(\"Type-Token Ratio (TTR):\", TTR)\n"
      ],
      "metadata": {
        "id": "eb9oBqo9cmNi",
        "outputId": "61060666-fb05-4a8a-fb20-827e35f5b128",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type-Token Ratio (TTR): 0.6129032258064516\n"
          ]
        }
      ]
    }
  ]
}